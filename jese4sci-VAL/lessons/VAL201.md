# VAL201: Unit testing

## Learning goals

-   Contrast *smoke* and *boundary* tests
-   Implement a unit test suite

## *"If you use software that lacks automated tests, you are the tests."*

[Jenny Bryan](https://twitter.com/jennybryan/status/1043307291909316609)

The goal of unit testing is to automate something you're already doing: poking your code to see if it works. This is normal and good for early exploration and prototyping, but as a project nears publication you're better off automating that process. Not only does it save you time, it frees up the mental space you were otherwise allocating to remembering your list of sanity checks. As Hadley Wickham (2011) put it:

> **Why test?**\
> I wrote **testthat** because I discovered I was spending too much time recreating bugs that I had previously fixed. While I was writing the original code or fixing the bug, I'd perform many interactive tests to make sure the code worked, but I never had a system for retaining these tests and running them, again and again. I think this is a common development practice of R programmers: it's not that we don't test our code, it's that we don't store our tests so they can be re-run automatically.

The biggest obstacle for automated testing is setting up the infrastructure to store and run your tests. The testthat package provides that infrastructure, but it requires you to develop your project as an R package. Fortunately, that's exactly what's happening under the hood when you use the research compendium framework provided by rrtools. So you're ready to get started!

## Types of tests

Testing is a software engineering discipline unto itself with all manner of goals and techniques. However, as a scientist, you'll want to focus on two types of tests: *smoke* and *boundary*.

Smoke tests validate basic functionality (i.e., sanity checks). They actually get their name from hardware testing. If you can plug something in and turn it on without smoke billowing out of it, you've passed your smoke test.

Boundary tests operate at the edges of your code. They validate that things still go as expected when you start giving functions unexpected inputs. Boundary tests tend to be trickier to write than smoke tests because they require you to think about all the possible things that *could* go wrong, rather than the few things that *should* go right.

Here's an example of a function with smoke and boundary test. `log10()` returns the base 10 logarithm of a number. A smoke test would check that `log10(10)` returns `1`. Boundary tests might check:

-   `log10(10^100)` returns `100` (the boundary being really, really big numbers)

-   `log10(-1)` returns `NaN` (negative input; NaN is R's representation of Not a Number)

-   `log10(NA)` returns `NA` (missing input)

-   `log10("foo")` throws an error (wrong type)

## Your turn

1.  Read [Chapter 12](https://r-pkgs.org/tests.html) of the R Packages book, up to section 12.3, to familiarize yourself with the testthat package.
2.  Go to GitHub and [fork](https://docs.github.com/en/get-started/quickstart/fork-a-repo) (i.e., create your own repo copy) [testme](https://github.com/FlukeAndFeather/testme).
3.  Open RStudio and create a new project from version control. Use the GitHub URL of *your fork* (not the original). testme is a dummy research compendium with one function. You're going to write some tests for it.
4.  Using the R Packages chapter on testing as a guide, write two smoke and four boundary tests for `binary_search()`. `binary_search()` doesn't handle boundary cases well, so you should be able to write at least two tests that fail.

## Submit assignment

Open an issue in [FlukeAndFeather/jese4sci-VAL](%5Bhttps://github.com/FlukeAndFeather/jese4sci-VAL).](<https://github.com/FlukeAndFeather/jese4sci-VAL>).) Put "VAL201" in the title. In the comment, include:

1.  A link to your fork of testme.
2.  Your test results (i.e., the output of `devtools::test()`).
