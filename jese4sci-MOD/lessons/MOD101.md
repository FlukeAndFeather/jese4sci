# MOD101: Reducing complexity

## Learning goals

1.  Critique "spaghetti" code
2.  Apply cognitive load concepts to writing and understanding code

## The problems with spaghetti code

> Thus, programs must be written for people to read, and only incidentally for machines to execute.

(Abelson and Sussman, 1996 p. *xvii*)

> It's important to remember that functions are not just for the computer, but are also for humans.

(Wickham and Grolemund, 2017 Ch. 19.3)

Most research-grade code looks like spaghetti. The flow of the code, its inputs and outputs, tend to jump around. By the time a paper is ready for publication, the code has grown organically, tangled like brambles. "Spaghetti code" is a problem for two reasons.

### 1. Spaghetti is hard to understand

Consider the following code. What's being accomplished?

``` {.r}
df <- data.frame(a = c(1, 5, 2, NA, 10))
df$a <- df$a - min(df$a, na.rm = TRUE)
df$a <- df$a / max(df$a, na.rm = TRUE)
```

The code above re-scales column `a` to [0, 1]. But it takes quite a lot of mental effort to figure that out! What makes this "spaghetti" is the re-scaling happens in two steps. Compare to the following.

``` {.r}
df <- data.frame(a = c(1, 5, 2, NA, 10))
rescale01 <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}
df$a <- rescale01(df$a)
```

This de-spaghettifies the code by extracting the re-scaling logic and putting it in its own function. Now the re-scaling happens in just one step *and* it has an easy to read name. `rescale01()` communicates the purpose of the code in a way that `df$a - min(df$a, na.rm = TRUE)` and so on does not.

### 2. Spaghetti is hard to reuse

Writing a `rescale01()` function might be overkill for just one column. But what if we need to repeat the task multiple times? A spaghetti version might look like this.

``` {.r}
df <- data.frame(x = c(1, 5, 2, NA, 10),
                 y = c(5, NA, 2, 10, 7),
                 z = c(2, 9, NA, 8, 7))
df$x <- df$x - min(df$x, na.rm = TRUE)
df$x <- df$x / max(df$x, na.rm = TRUE)
df$y <- df$y - min(df$y, na.rm = TRUE)
df$y <- df$y / max(df$y, na.rm = TRUE)
df$z <- df$z - min(df$z, na.rm = TRUE)
df$z <- df$z / max(df$z, na.rm = TRUE)
```

That's a lot of repetition, which not only takes more work, it also means a lot of opportunities for typos and bugs. The de-spaghettified code is much cleaner. As an additional bonus, it's now easy to share `rescale01()` with your labmates and collaborators.

``` {.r}
df <- data.frame(x = c(1, 5, 2, NA, 10),
                 y = c(5, NA, 2, 10, 7),
                 z = c(2, 9, NA, 8, 7))
rescale01 <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}
df$x <- rescale01(df$x)
df$y <- rescale01(df$y)
df$z <- rescale01(df$z)
```

Granted, re-scaling a vector is a relatively trivial task. But you can imagine the issues that arise when you start incorporating multiple data sources, statistical models, etc.

## Cognitive load

The biggest reason spaghetti code is so dangerous is it lacks internal structure, forcing the programmer to hold much more context in memory at once. When code is divided into distinct modules (e.g., functions, packages, etc), the programmer can ignore most of it and just focus on the task at hand. This speeds up development, reduces bugs, and produces code that is easier to read, maintain, and share. Jamie Matthews explained the connections between programming and cognitive load in an [insightful blog post](https://www.dabapps.com/blog/cognitive-load-programming/).

> ### What is cognitive load?
>
> *Cognitive load* is a concept from cognitive psychology, originally developed by John Sweller, that is generally discussed in relation to learning. It is a measure of the amount of effort being used in the working memory. Working memory is, loosely speaking, the part of the cognitive system that processes information related to what the individual is currently doing (in contrast to long-term memory, which retains structured knowledge). We know that working memory is quite limited in its capacity (try remembering a phone number after hearing it once) and is vulnerable to overload.
>
> Programming and related tasks (like tracking down bugs) are very reliant on working memory. As programmers we're familiar with the feeling of "loading a program into your brain", of holding a subset of an algorithm or data model in your mind so you can start to reason about it. This happens whenever we're asked to look at a new, unfamiliar piece of code (if we've just started working at a new company, say). But it also happens with software we've been maintaining for years. Any non-trivial software system is so complex that no individual developer can understand or remember how the whole thing works all of the time, so we have to re-familiarise ourselves with the inner workings of a particular part of the codebase before we can start making changes or fixing bugs. Maintaining software is essentially a continual learning process, so the theoretical underpinnings of cognitive load can be applied.
>
> Cognitive load, according to the psychologists, falls into three types:
>
> **Intrinsic cognitive load** is the difficultly of learning the task itself. When learning how to make a cake, say, it would be the effort required to weigh and combine all of the ingredients correctly, stir the batter just the right amount, preheat the oven, time the baking correctly and so on.
>
> **Germane cognitive load** is effort spent formulating new lower-level representations of sub-parts of the task. Part of the recipe might be spent explaining how to make icing in great detail. Over time, however, an experienced baker will learn exactly how to make icing, and so that part of the overall cake-making task will be reduced to simply "make the icing".
>
> **Extrinsic cognitive load** is additional effort imposed by the manner in which the learning takes place. In the cake example, imagine if all of the units are given in pounds and ounces but you are accustomed onto to working in grams and kilograms. Extrinsic load would be the time spend converting between imperial and metric at every step of the way.
>
> Simply speaking, intrinsic and germane load are considered "good" and extrinsic "bad". Too much germane load makes learning a new task initially harder and slower (while the learner picks up the underlying concepts), but this work is valuable because the next time you come to do the same task, it'll be easier. Intrinsic load is just the effort required to perform the task - but this should reduce over time as new lower-level knowledge and experience are formed. Extrinsic load is like friction, slowing you down and filling up your working memory with non-essential, wasted effort.
>
> Hopefully, if you're a programmer, you'll already have an intuitive sense that these types of cognitive load have some relevance in your day-to-day tasks. The key argument of this article is that we can (and should) examine programming constructs and abstractions by considering the effects they have on intrinsic, extrinsic or germane cognitive load in future readers of the code.

## Submit assignment

Open an issue in [FlukeAndFeather/jese4sci](https://github.com/FlukeAndFeather/jese4sci). Put "MOD101" in the title. In the comment, answer the following questions.

1.  Find a piece of spaghetti code you wrote. What are one or two ways you could de-spaghettify it? You don't need to re-write the code, just describe how you would restructure it.
2.  Reading the same piece of code, describe one example each of intrinsic, germane, and extrinsic load. These are subjective categories, so explain your answers. For example, in the `rescale01()` code, I would argue:

    - The `rescale01()` function as a whole is intrinsic load because it performs the task we care about
    - Calling the function is germane load, because we've abstracted the task into something simpler
    - The repeated `na.rm = TRUE` argument is extrinsic load because it's verbose and not directly related to the purpose of the function

## References

Abelson, H., & Sussman, G. J. (1996). *Structure and interpretation of computer programs*. The MIT Press.

Wickham, H., & Grolemund, G. (2017). *R for data science: import, tidy, transform, visualize, and model data*. O'Reilly Media, Inc.
